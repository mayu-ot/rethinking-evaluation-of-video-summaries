---
layout: page
title: Rethinking the Evaluation of Video Summaries
subtitle: Mayu Otani<sup>1</sup>, Yuta Nakahima<sup>2</sup>, Esa Rahtu<sup>3</sup>, and Janne Heikkilä<sup>4</sup><br><sup>1</sup>CyberAgent, Inc. <sup>2</sup>Osaka University <sup>3</sup>Tampere University <sup>4</sup>University of Oulu
use-site-title: true
---
<script async src="//cdn.embedly.com/widgets/platform.js" charset="UTF-8"></script>

<h2>Short Introduction</h2>
<p>
We provide in-depth assessment of the evaluation pipeline for video summaries using two popular benchmark datasets: SumMe <a href="#fn1">[1]</a> and TVSum <a href="#fn2">[2]</a> . Surprisingly, we observe that randomly generated summaries achieve comparable or better performance to the state-of-the-art. In some cases, the random summaries outperform even the reference summaries in leave-one-out experiments. Moreover, it turns out that the video segmentation, which is often considered as a fixed pre-processing method, has the most significant impact on the performance measure.
</p>

<small>
<ol>
    <li id="fn1">
        M. Gygli, H. Grabner, H. Riemenschneider, and L. van Gool, “Creating summaries from user videos,” in European Conference on Computer Vision (ECCV), 2014, pp. 505–520.
    </li>
    <li id="fn2">
        Y. Song, J. Vallmitjana, A. Stent, and A. Jaimes, “TVSum : Summarizing Web Videos Using Titles,” in IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 5179–5187.
    </li>
</ol>
</small>

<h2>Results</h2>

<div style="text-align: center">
<img src="assets/images/summe_random_eval.png" width=400>
</div>
<blockquote>
        F1 scores for different segmentation and importance score combinations for SumMe. Light blue bars refer to random summaries and dark blue bars indicate scores of manually created reference summaries (leave-one-out test). Purple bars show the scores for DR-DSN importance scoring with different segmentation methods.
</blockquote>

<div style="text-align: center">
<img src="assets/images/tvsum_random_eval.png" width=400>
</div>
<blockquote>
        F1 scores for different segmentation methods combined to either random or human annotated importance scores (leave-one-out) for TVSum dataset. Light blue bars refer to random scores and dark blue bars indicates human annotations. Interestingly, the random and human annotations obtain similar F1 scores in most cases.
</blockquote>

<h2>Paper</h2>
<a class="embedly-card" href="https://arxiv.org/abs/1903.11328">Rethinking the Evaluation of Video Summaries</a>

<h2>Code</h2>
<a class="embedly-card" href="https://github.com/mayu-ot/rethinking-evs">mayu-ot/rethinking-evs</a>

<h2>Citation</h2>

<pre>
@InProceedings{otani2018vsumeval,
    title={Rethinking the Evaluation of Video Summaries},
    author={Mayu Otani, Yuta Nakahima, Esa Rahtu, and Janne Heikkil{\"{a}}},
    booktitle={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2019}
}
</pre>

